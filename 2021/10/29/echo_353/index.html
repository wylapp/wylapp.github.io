<!DOCTYPE html>
<html lang="zh-cn">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
    
    
    
    


    <!-- meta -->


<title>word2vec简单实现 | My::tech</title>





    <!-- OpenGraph -->
 
    <meta name="description" content="## 梗概 word2vec算是embedding中近年来比较火的方法了。从实践角度来讲有CBOW和skip-gram两套思路，一种是从context推断local，而另一种是从local推断context。但是我们其实并不关心模型的输出，而只关心模型内部的隐层。">
<meta property="og:type" content="article">
<meta property="og:title" content="word2vec简单实现">
<meta property="og:url" content="https://wylapp.github.io/2021/10/29/echo_353/index.html">
<meta property="og:site_name" content="My::tech">
<meta property="og:description" content="## 梗概 word2vec算是embedding中近年来比较火的方法了。从实践角度来讲有CBOW和skip-gram两套思路，一种是从context推断local，而另一种是从local推断context。但是我们其实并不关心模型的输出，而只关心模型内部的隐层。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://generalapi.top/wp-content/uploads/2021/10/image-1635477919607.png">
<meta property="article:published_time" content="2021-10-29T03:27:00.000Z">
<meta property="article:modified_time" content="2023-07-20T02:45:34.322Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://generalapi.top/wp-content/uploads/2021/10/image-1635477919607.png">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
        <link rel="stylesheet" id="hl-default-theme" href="/css/highlight/default.css" media="none" >
        
            <link rel="stylesheet" id="hl-dark-theme" href="/css/highlight/dark.css" media="none">
        
    

    
    

    
    
<link rel="stylesheet" href="/css/style/dark.css">

    
<script src="/js/darkmode.js"></script>



     

    <!-- custom head -->

<meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <div id="app" tabindex="-1">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">My::tech</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">首页</a>
                
                    <a href="/tags/" class="navbar-menu button">标签</a>
                
                    <a href="/archives/" class="navbar-menu button">归档</a>
                
            </div>
        
        
        
    <a href="/search/" id="btn-search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="24" height="24" fill="currentColor" stroke="currentColor" stroke-width="32"><path d="M192 448c0-141.152 114.848-256 256-256s256 114.848 256 256-114.848 256-256 256-256-114.848-256-256z m710.624 409.376l-206.88-206.88A318.784 318.784 0 0 0 768 448c0-176.736-143.264-320-320-320S128 271.264 128 448s143.264 320 320 320a318.784 318.784 0 0 0 202.496-72.256l206.88 206.88 45.248-45.248z"></path></svg>
    </a>


        
        
    <a href="javaScript:void(0);" id="btn-toggle-dark">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">首页</a>
                
                    <a href="/tags/" class="dropdown-menu button">标签</a>
                
                    <a href="/archives/" class="dropdown-menu button">归档</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        word2vec简单实现
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2021/10/" class="post-meta__date button">2021-10-29</a>
        
 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">核心代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">结果</span></a></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">文章目录</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">核心代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">结果</span></a></li></ol>
    </div>


<article class="post post__with-toc content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <!--markdown-->## 梗概
<p>word2vec算是embedding中近年来比较火的方法了。从实践角度来讲有<code>CBOW</code>和<code>skip-gram</code>两套思路，一种是从context推断local，而另一种是从local推断context。但是我们其实并不关心模型的输出，而只关心模型内部的<strong>隐层</strong>。</p>
<span id="more"></span>
<h2 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h2><p>使用pytorch构建了一套标准pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WordsVec</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, datafile=<span class="string">&quot;dataset/sentences.txt&quot;</span>, window=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(WordsVec, self).__init__()</span><br><span class="line">        self.datafile = datafile</span><br><span class="line">        self.tokens = self._unique_words()</span><br><span class="line">        self.window = window</span><br><span class="line">        self.tokensize = <span class="built_in">len</span>(self.tokens)</span><br><span class="line">        self.paired_dataset = self._prepare_dataset(<span class="string">&quot;kgram&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_unique_words</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.datafile, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            ts = fp.read()</span><br><span class="line">        self.sentences = ts.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        self.tokenized_sentences = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>), self.sentences))</span><br><span class="line">        tokens = <span class="built_in">list</span>(<span class="built_in">set</span>(reduce(<span class="keyword">lambda</span> x, y: x+y, self.tokenized_sentences)))</span><br><span class="line">        self.token_id_dict = &#123;token: ids <span class="keyword">for</span> ids, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens)&#125;</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_prepare_dataset</span>(<span class="params">self, *args</span>):</span><br><span class="line">        dataset = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> self.tokenized_sentences:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) &amp;lt; <span class="number">2</span>*self.window + <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> ps <span class="keyword">in</span> <span class="built_in">range</span>(self.window, <span class="built_in">len</span>(line)-self.window):</span><br><span class="line">                    <span class="comment"># ps is the pointer to the current position, which has inner-padding size of `self.window`</span></span><br><span class="line">                    target = line[ps]</span><br><span class="line">                    upper_pointer = downer_point = ps</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.window+<span class="number">1</span>):</span><br><span class="line">                        <span class="keyword">if</span> args[<span class="number">0</span>] == <span class="string">&quot;kgram&quot;</span>:</span><br><span class="line">                            dataset.append([self._one_hot_word(target),</span><br><span class="line">                                            self.word2id(line[upper_pointer+i])])</span><br><span class="line">                            dataset.append([self._one_hot_word(target),</span><br><span class="line">                                            self.word2id(line[downer_point-i])])</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            dataset.append([self._one_hot_word(line[upper_pointer+i]),</span><br><span class="line">                                            self._one_hot_word(target)])</span><br><span class="line">                            dataset.append([self._one_hot_word(line[downer_point-i]),</span><br><span class="line">                                            self._one_hot_word(target)])</span><br><span class="line">        <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_one_hot</span>(<span class="params">self, index</span>):</span><br><span class="line">        hotvector = torch.zeros(self.maxwordlen)</span><br><span class="line">        hotvector[index] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> hotvector</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_one_hot_word</span>(<span class="params">self, word</span>):</span><br><span class="line">        hotvector = torch.zeros(self.maxwordlen)</span><br><span class="line">        hotvector[self.token_id_dict[word]] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> hotvector</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">id2word</span>(<span class="params">self, ids</span>):</span><br><span class="line">        <span class="keyword">return</span> self.tokens[ids]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">word2id</span>(<span class="params">self, word</span>):</span><br><span class="line">        <span class="keyword">return</span> self.token_id_dict[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.paired_dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        <span class="keyword">return</span> self.paired_dataset[item]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxwordlen</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.tokensize</span><br><span class="line"></span><br><span class="line">train_set = WordsVec()</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoProjection</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, word_len, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoProjection, self).__init__()</span><br><span class="line">        self.W = nn.Parameter(torch.randn(word_len, hidden_size))</span><br><span class="line">        self.V = nn.Parameter(torch.randn(hidden_size, word_len))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hot_vec</span>):</span><br><span class="line">        hidden_layer = torch.matmul(hot_vec, self.W)</span><br><span class="line">        output_layer = torch.matmul(hidden_layer, self.V)</span><br><span class="line">        <span class="keyword">return</span> output_layer</span><br><span class="line"></span><br><span class="line">twoproj = TwoProjection(train_set.maxwordlen, <span class="number">2</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(twoproj.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trainModel</span>():</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, datax <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        inputs, label = datax</span><br><span class="line">        pred = twoproj(inputs)</span><br><span class="line">        loss = criterion(pred, label)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, total_loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> epochs <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epochs))</span><br><span class="line">        trainModel()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_set.tokens):</span><br><span class="line">        W, WT = twoproj.parameters()</span><br><span class="line">        <span class="comment"># W是词向量矩阵</span></span><br><span class="line">        x, y = <span class="built_in">float</span>(W[i][<span class="number">0</span>]), <span class="built_in">float</span>(W[i][<span class="number">1</span>])</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">&#x27;offset points&#x27;</span>, ha=<span class="string">&#x27;right&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>这套代码对于网络核心层采用了网上提供的版本，采用了<code>nn.Parameter</code>模块。最早的版本使用了<code>torch.nn.Linear</code>来完成核心投影层的构建，因为Linear模块是可以定义输入维数和输出维数的，对于$y&#x3D;wx$这样的线性回归，如果$y\in \mathbb{R^v}, x\in \mathbb{R^n}$, 那么只要$w \in \mathbb{R^{n\times v}}$就能完成维度的映射。实际上使用线形层是没有问题的，只不过最后输出词向量的时候需要使用第一层的权重矩阵点乘词语的onehot向量。下面给出修改后的部分代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structure of the Network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoProjection</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, word_len, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoProjection, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(word_len, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.l2 = nn.Linear(hidden_size, word_len, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hot_vec</span>):</span><br><span class="line">        hidden_layer = self.l1(hot_vec)</span><br><span class="line">        output_layer = self.l2(hidden_layer)</span><br><span class="line">        <span class="keyword">return</span> output_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert a word into vector and visualization</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># training here</span></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_set.tokens):</span><br><span class="line">        W = twoproj.l1.weight.data</span><br><span class="line">        b = twoproj.l1.bias.data</span><br><span class="line">        x, y = torch.matmul(W, train_set._one_hot_word(label))</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">&#x27;offset points&#x27;</span>, ha=<span class="string">&#x27;right&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2>结果</h2>
尝试在模型中开启Linear层中加入偏置量，发现也可以得到预期的结果！
<img src="https://generalapi.top/wp-content/uploads/2021/10/image-1635477919607.png" alt="file">
这是代码运行的结果。其中使用Linear模块epoch在200就可以达到很好的映射效果，但是如果使用Parameter模块，达到同样的映射效果至少需要linear两倍的epoch。
    </div>
    
    <div class="post__license">
        <p>
            <strong>本文作者：</strong>ChrAlpha
        </p>
        <p>
            <strong>
                本文链接：
            </strong>
            <a href="https://wylapp.github.io/2021/10/29/echo_353/">https://wylapp.github.io/2021/10/29/echo_353/</a>
        </p>
        
    </div>
 
    <div class="post-footer__meta"><p>更新于 2023-07-20</p></div> 
    <div class="post-entry__tags"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/2021/11/06/echo_363/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            上一篇
                        </div>
                        <div class="nav__title">
                            axios(或jQuery)异步下载文件的有效方法
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/2021/07/17/echo_258/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            下一篇
                        </div>
                        <div class="nav__title">
                            uWSGI应用发布随笔
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
     
    <a href="#" class="button" id="b2t" aria-label="回到顶部" title="回到顶部">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>

    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2023 <a href="/">My::tech</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

 



 



 


    
 


    
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.4.1/dist/jquery.fancybox.min.css">

    
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.4.1/dist/jquery.fancybox.min.js"></script>

    <script>
        let lazyloadT = Boolean('false'),
            auto_fancybox = Boolean('false')
        if (auto_fancybox) {
            $(".post__content").find('img').each(function () {
                var element = document.createElement("a");
                $(element).attr("data-fancybox", "gallery");
                $(element).attr("href", $(this).attr("src"));
                if (lazyloadT) {
                    $(element).attr("href", $(this).attr("data-srcset"));
                }
                $(this).wrap(element);
            });
        } else {
            $(".post__content").find("fancybox").find('img').each(function () {
                var element = document.createElement("a");
                $(element).attr("data-fancybox", "gallery");
                $(element).attr("href", $(this).attr("src"));
                if (lazyloadT) {
                    $(element).attr("href", $(this).attr("data-srcset"));
                }
                $(this).wrap(element);
            });
        }
    </script>
 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: {'[+]': [['$', '$']]},
                    tags: 'ams'
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 

 




    </body>
</html>
